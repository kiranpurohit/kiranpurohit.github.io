---
title: "Winning the Lottery Ahead of Time: Efficient Early Network Pruning @CNeRG Reading Group"
event: CNeRG Reading Group 
event_url: 


# summary: ""
abstract: "Pruning, the task of sparsifying deep neural networks, received increasing attention recently. Although state-of-the-art pruning methods extract highly sparse models, they neglect two main challenges: (1) the process of finding these sparse models is often very expensive; (2) unstructured pruning does not provide benefits in terms of GPU memory, training time, or carbon emissions. We propose Early Compression via Gradient Flow Preservation (EarlyCroP), which efficiently extracts state-of-the-art sparse models before or early in training addressing challenge (1), and can be applied in a structured manner addressing challenge (2). This enables us to train sparse networks on commodity GPUs whose dense versions would be too large, thereby saving costs and reducing hardware requirements. We empirically show that EarlyCroP outperforms a rich set of baselines for many tasks (incl. classification, regression) and domains (incl. computer vision, natural language processing, and reinforcment learning). EarlyCroP leads to accuracy comparable to dense training while outperforming pruning baselines." 


# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: 2023-06-08


# Schedule page publish date (NOT talk date).
# <!-- publishDate: 2021-02-11T08:15:38+05:30 -->

authors: ['Kiran Purohit']
# tags: []

# Is this a featured talk? (true/false)
# featured: true
# selected: true

image:
  caption: ''
  focal_point: Smart

links:
url_code: ""
url_pdf: "https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf"
url_slides: "Winning_the_lottery_ahead_of_time.pdf"
url_video: ""



slides: ""



# Enable math on this page?
math: true
---


