---
title: "LearnDefend: Learning to Defend against Backdoor Attacks on Federated Learning @AIMLSystems Doctoral Symposium 2022"
event: AIMLSystems Conference
event_url: https://www.aimlsystems.org/2022/


# summary: ""
abstract: "Federated Learning has emerged as an important paradigm for training Machine Learning (ML) models. The key idea is that many clients own the data needed to train their local ML models, and share the local models with a master, which in turn shares the aggregated global model back with each of the clients. The federated averaging algorithm has been a mainstay of federated learning, due to its effectiveness, simplicity, and privacy preserving properties. However, they have been shown to be particularly vulnerable to model-poisoning attacks by one or more clients. Two particular properties of modern model-poisoning attacks make them virtually undetectable. Firstly, the model-replacement attacks proposed in, can offset the “correct” models contributed by many other clients, even under bounded-deviation constraints. Secondly, edge-case backdoor attacks, can manifest themselves on a very small subset of the input feature space. These factors lead Wang et al. to lead to the conclusion that no fixed defense rule can stop the backdoor attack on federated learning system"


# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: 2022-10-14


# Schedule page publish date (NOT talk date).
# <!-- publishDate: 2021-02-11T08:15:38+05:30 -->

authors: ['Kiran Purohit']
# tags: []

# Is this a featured talk? (true/false)
# featured: true
# selected: true

image:
  caption: ''
  focal_point: Smart

links:
url_code: ""
url_pdf: "https://arxiv.org/pdf/2305.02022.pdf"
url_slides: "AIML_Symposium_presentation_slides.pdf"
url_video: "https://www.youtube.com/watch?v=ToMKDGL_0gM"



slides: ""



# Enable math on this page?
math: true
---


