<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TensorBoard | Kiran Purohit</title>
    <link>https://kiranpurohit.github.io/tag/tensorboard/</link>
      <atom:link href="https://kiranpurohit.github.io/tag/tensorboard/index.xml" rel="self" type="application/rss+xml" />
    <description>TensorBoard</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 08 Nov 2017 15:09:44 +0530</lastBuildDate>
    <image>
      <url>https://kiranpurohit.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>TensorBoard</title>
      <link>https://kiranpurohit.github.io/tag/tensorboard/</link>
    </image>
    
    <item>
      <title>TensorBoard Basics</title>
      <link>https://kiranpurohit.github.io/news/tensorboard_basics/</link>
      <pubDate>Wed, 08 Nov 2017 15:09:44 +0530</pubDate>
      <guid>https://kiranpurohit.github.io/news/tensorboard_basics/</guid>
      <description>&lt;h2 id=&#34;what-is-tensorboard&#34;&gt;What is TensorBoard?&lt;/h2&gt;
&lt;p&gt;TensorBoard is a module of Tensorflow that provides a suite of visualisation tools that help in understanding, debugging and optimize the model created in Tensorflow.&lt;/p&gt;
&lt;h2 id=&#34;installing-tensorboard&#34;&gt;Installing TensorBoard&lt;/h2&gt;
&lt;p&gt;TensorBoard is installed along with Tensorflow.
It can also be installed as a standalone software from 
&lt;a href=&#34;https://github.com/dmlc/tensorboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standalone TensorBoard&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;p&gt;TensorBoard works by reading summary data from Tensorflow event files which is generated by tensorflow.&lt;/p&gt;
&lt;p&gt;Tensorflow creates a computational graph of the neural network model implemented.This model is then stored as a graph using the &lt;strong&gt;summary&lt;/strong&gt; method.The name field in the Tensorflow &lt;strong&gt;variables&lt;/strong&gt;, &lt;strong&gt;placeholders&lt;/strong&gt;, &lt;strong&gt;operations&lt;/strong&gt;, and &lt;strong&gt;name_scopes&lt;/strong&gt; are used to annotate the nodes in the graph. These graphs are written to files and stored in a directory indicated in the &lt;strong&gt;summary.FileWriter&lt;/strong&gt; method.We can select which nodes to be included in the graph using the &lt;strong&gt;summary.merge&lt;/strong&gt; method. In order to select all the nodes to be included in the graph by using the &lt;strong&gt;summary.merge_all&lt;/strong&gt; method.This summary (in the form of a protocol buffer) is added to the event file using the &lt;strong&gt;add_summary&lt;/strong&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#Although the model in this code-segment will not converge, 
#this is just to show how tensorflow creates and saves summaries 
#in event files.
import tensorflow as tf
import numpy as np
N = 5
x = tf.placeholder(tf.float32, shape=[N,1],name=&#39;X&#39;) 
y = tf.placeholder(tf.float32, shape=[N,N],name=&#39;y&#39;) 
with tf.name_scope(&#39;fc_layer&#39;):
    W = tf.Variable(tf.truncated_normal([1,N], stddev=0.1), name=&#39;W&#39;)
    b = tf.Variable(tf.constant(0.1,shape=[N]),name=&#39;b&#39;)
    tf.summary.histogram(&#39;weights&#39;,W) #values of W will be shown as a histogram plot
    tf.summary.histogram(&#39;biases&#39;,b) #values of W will be shown as a histogram plot
    y_ = tf.matmul(x,W) + b
with tf.name_scope(&#39;loss_func&#39;):    
    
    tf.summary.scalar(&#39;loss&#39;,0.05)


sess = tf.Session()
merged_summary = tf.summary.merge_all()
writer = tf.summary.FileWriter(&amp;quot;./graphs/&amp;quot;, graph=sess.graph)
sess.run(tf.global_variables_initializer())
s = sess.run(merged_summary,feed_dict={x:np.ones((N,1)),y:np.ones((N,N))})
i = 1  
writer.add_summary(s,i)
```python

## Running TensorBoard

Now that the event files have been created and stored in the directory[./graphs], running tensorboard with logdir pointing to the directory where the event files have been stored will give us a visualization of the model in the web browser.
```python
tensorboard --logdir=./graphs
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>TensorBoard Basics</title>
      <link>https://kiranpurohit.github.io/post/tensorboard_basics/</link>
      <pubDate>Wed, 08 Nov 2017 15:09:44 +0530</pubDate>
      <guid>https://kiranpurohit.github.io/post/tensorboard_basics/</guid>
      <description>&lt;h2 id=&#34;what-is-tensorboard&#34;&gt;What is TensorBoard?&lt;/h2&gt;
&lt;p&gt;TensorBoard is a module of Tensorflow that provides a suite of visualisation tools that help in understanding, debugging and optimize the model created in Tensorflow.&lt;/p&gt;
&lt;h2 id=&#34;installing-tensorboard&#34;&gt;Installing TensorBoard&lt;/h2&gt;
&lt;p&gt;TensorBoard is installed along with Tensorflow.
It can also be installed as a standalone software from 
&lt;a href=&#34;https://github.com/dmlc/tensorboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standalone TensorBoard&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;p&gt;TensorBoard works by reading summary data from Tensorflow event files which is generated by tensorflow.&lt;/p&gt;
&lt;p&gt;Tensorflow creates a computational graph of the neural network model implemented.This model is then stored as a graph using the &lt;strong&gt;summary&lt;/strong&gt; method.The name field in the Tensorflow &lt;strong&gt;variables&lt;/strong&gt;, &lt;strong&gt;placeholders&lt;/strong&gt;, &lt;strong&gt;operations&lt;/strong&gt;, and &lt;strong&gt;name_scopes&lt;/strong&gt; are used to annotate the nodes in the graph. These graphs are written to files and stored in a directory indicated in the &lt;strong&gt;summary.FileWriter&lt;/strong&gt; method.We can select which nodes to be included in the graph using the &lt;strong&gt;summary.merge&lt;/strong&gt; method. In order to select all the nodes to be included in the graph by using the &lt;strong&gt;summary.merge_all&lt;/strong&gt; method.This summary (in the form of a protocol buffer) is added to the event file using the &lt;strong&gt;add_summary&lt;/strong&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#Although the model in this code-segment will not converge, 
#this is just to show how tensorflow creates and saves summaries 
#in event files.
import tensorflow as tf
import numpy as np
N = 5
x = tf.placeholder(tf.float32, shape=[N,1],name=&#39;X&#39;) 
y = tf.placeholder(tf.float32, shape=[N,N],name=&#39;y&#39;) 
with tf.name_scope(&#39;fc_layer&#39;):
    W = tf.Variable(tf.truncated_normal([1,N], stddev=0.1), name=&#39;W&#39;)
    b = tf.Variable(tf.constant(0.1,shape=[N]),name=&#39;b&#39;)
    tf.summary.histogram(&#39;weights&#39;,W) #values of W will be shown as a histogram plot
    tf.summary.histogram(&#39;biases&#39;,b) #values of W will be shown as a histogram plot
    y_ = tf.matmul(x,W) + b
with tf.name_scope(&#39;loss_func&#39;):    
    
    tf.summary.scalar(&#39;loss&#39;,0.05)


sess = tf.Session()
merged_summary = tf.summary.merge_all()
writer = tf.summary.FileWriter(&amp;quot;./graphs/&amp;quot;, graph=sess.graph)
sess.run(tf.global_variables_initializer())
s = sess.run(merged_summary,feed_dict={x:np.ones((N,1)),y:np.ones((N,N))})
i = 1  
writer.add_summary(s,i)
```python

## Running TensorBoard

Now that the event files have been created and stored in the directory[./graphs], running tensorboard with logdir pointing to the directory where the event files have been stored will give us a visualization of the model in the web browser.
```python
tensorboard --logdir=./graphs
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
