<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python | Kiran Purohit</title>
    <link>https://kiranpurohit.github.io/tag/python/</link>
      <atom:link href="https://kiranpurohit.github.io/tag/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Python</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 08 Nov 2017 15:09:44 +0530</lastBuildDate>
    <image>
      <url>https://kiranpurohit.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Python</title>
      <link>https://kiranpurohit.github.io/tag/python/</link>
    </image>
    
    <item>
      <title>TensorBoard Basics</title>
      <link>https://kiranpurohit.github.io/post/tensorboard_basics/</link>
      <pubDate>Wed, 08 Nov 2017 15:09:44 +0530</pubDate>
      <guid>https://kiranpurohit.github.io/post/tensorboard_basics/</guid>
      <description>&lt;h2 id=&#34;what-is-tensorboard&#34;&gt;What is TensorBoard?&lt;/h2&gt;
&lt;p&gt;TensorBoard is a module of Tensorflow that provides a suite of visualisation tools that help in understanding, debugging and optimize the model created in Tensorflow.&lt;/p&gt;
&lt;h2 id=&#34;installing-tensorboard&#34;&gt;Installing TensorBoard&lt;/h2&gt;
&lt;p&gt;TensorBoard is installed along with Tensorflow.
It can also be installed as a standalone software from 
&lt;a href=&#34;https://github.com/dmlc/tensorboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standalone TensorBoard&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;p&gt;TensorBoard works by reading summary data from Tensorflow event files which is generated by tensorflow.&lt;/p&gt;
&lt;p&gt;Tensorflow creates a computational graph of the neural network model implemented.This model is then stored as a graph using the &lt;strong&gt;summary&lt;/strong&gt; method.The name field in the Tensorflow &lt;strong&gt;variables&lt;/strong&gt;, &lt;strong&gt;placeholders&lt;/strong&gt;, &lt;strong&gt;operations&lt;/strong&gt;, and &lt;strong&gt;name_scopes&lt;/strong&gt; are used to annotate the nodes in the graph. These graphs are written to files and stored in a directory indicated in the &lt;strong&gt;summary.FileWriter&lt;/strong&gt; method.We can select which nodes to be included in the graph using the &lt;strong&gt;summary.merge&lt;/strong&gt; method. In order to select all the nodes to be included in the graph by using the &lt;strong&gt;summary.merge_all&lt;/strong&gt; method.This summary (in the form of a protocol buffer) is added to the event file using the &lt;strong&gt;add_summary&lt;/strong&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#Although the model in this code-segment will not converge, 
#this is just to show how tensorflow creates and saves summaries 
#in event files.
import tensorflow as tf
import numpy as np
N = 5
x = tf.placeholder(tf.float32, shape=[N,1],name=&#39;X&#39;) 
y = tf.placeholder(tf.float32, shape=[N,N],name=&#39;y&#39;) 
with tf.name_scope(&#39;fc_layer&#39;):
    W = tf.Variable(tf.truncated_normal([1,N], stddev=0.1), name=&#39;W&#39;)
    b = tf.Variable(tf.constant(0.1,shape=[N]),name=&#39;b&#39;)
    tf.summary.histogram(&#39;weights&#39;,W) #values of W will be shown as a histogram plot
    tf.summary.histogram(&#39;biases&#39;,b) #values of W will be shown as a histogram plot
    y_ = tf.matmul(x,W) + b
with tf.name_scope(&#39;loss_func&#39;):    
    
    tf.summary.scalar(&#39;loss&#39;,0.05)


sess = tf.Session()
merged_summary = tf.summary.merge_all()
writer = tf.summary.FileWriter(&amp;quot;./graphs/&amp;quot;, graph=sess.graph)
sess.run(tf.global_variables_initializer())
s = sess.run(merged_summary,feed_dict={x:np.ones((N,1)),y:np.ones((N,N))})
i = 1  
writer.add_summary(s,i)
```python

## Running TensorBoard

Now that the event files have been created and stored in the directory[./graphs], running tensorboard with logdir pointing to the directory where the event files have been stored will give us a visualization of the model in the web browser.
```python
tensorboard --logdir=./graphs
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow Basics</title>
      <link>https://kiranpurohit.github.io/post/tensorflow_basics/</link>
      <pubDate>Wed, 05 Jul 2017 21:25:58 +0530</pubDate>
      <guid>https://kiranpurohit.github.io/post/tensorflow_basics/</guid>
      <description>&lt;h1 id=&#34;what-is-tensorflow&#34;&gt;What is Tensorflow?&lt;/h1&gt;
&lt;p&gt;Tensorflow is an open source software for machine learning developed by Google.Tensorflow, as one can get from its name, mainly handles matrices(or tensors), its mathematical operations and differentiation efficiently.It is just like Theano but with some extra features like  it can be used on distributed systems.&lt;/p&gt;
&lt;h2 id=&#34;installing-tensorflow&#34;&gt;Installing Tensorflow&lt;/h2&gt;
&lt;p&gt;Installing tensorflow &amp;gt;= 1.1.0 can be done using python-pip&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade tensorflow # for the CPU only version
pip install --upgrade tensorflow-gpu # for the GPU version
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;key-components&#34;&gt;Key-Components&lt;/h2&gt;
&lt;p&gt;Tensorflow works by first creating a computational graph resembling the model we wish to run and then executing it.
A program written using tensorflow must consist of the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Variables&lt;/strong&gt;: variables in tensorflow are in memory buffers containing tensors,but unlike normal tensors that live only for a single execution of a graph, variables values live as long as the session exists.Variables value cease to exist after the session is closed. Tensorflow has the option of saving variables&amp;rsquo; value to disk and restoring them for later use.Variables must be initialized before executing a graph for first time.
During training operation variables get updated by default.Tensorflow variables are created using &lt;strong&gt;tf.Variable()&lt;/strong&gt;.We can keep a variable unchanged by explicitly setting its trainable parameter to false.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
&amp;gt;&amp;gt;&amp;gt; # Declaring a tensoflow Variable
&amp;gt;&amp;gt;&amp;gt; W = tf.Variable(tf.random_uniform([2,3],stddev=0.5), # same a numpy.random.uniform
name=&#39;weight&#39;) # name of the variable in the computational graph
&amp;gt;&amp;gt;&amp;gt; # Setting variable b to non -trainable
&amp;gt;&amp;gt;&amp;gt; b = tf.Variable(tf.zeros([1]), # same as numpy.zeros
name=&#39;b&#39;, trainable=False)
&amp;gt;&amp;gt;&amp;gt; s = tf.Session() # session created
&amp;gt;&amp;gt;&amp;gt; s.run(tf.initialize_all_variables()) # To initialize all the variables present in the current session
&amp;gt;&amp;gt;&amp;gt; s.run(tf.initialize_variables(W)) # To only initialize W and not b
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Placeholders&lt;/strong&gt;: A placeholder can be thought of as a variable to which we can assign data at a later step.It is populated every single time a computational graph is run.A placeholder usually holds the input values to a model.Placeholder is created in tensorflow using &lt;strong&gt;tf.placeholder()&lt;/strong&gt;.Values in placeholders are entered using a feed_dict argument.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
&amp;gt;&amp;gt;&amp;gt; # Declaring a tensoflow Variable
&amp;gt;&amp;gt;&amp;gt; W = tf.Variable(tf.random_uniform([2,3],stddev=0.5), # same a numpy.random.uniform
name=&#39;weight&#39;) # name of the variable in the computational graph
&amp;gt;&amp;gt;&amp;gt; # Setting variable b to non -trainable
&amp;gt;&amp;gt;&amp;gt; b = tf.Variable(tf.zeros([3]), # same as numpy.zeros
name=&#39;b&#39;, trainable=False)
&amp;gt;&amp;gt;&amp;gt; x = tf.placeholder(tf.float32, name=&#39;x&#39;, shape=[None, 2]) # defining a placeholder named x
&amp;gt;&amp;gt;&amp;gt; # of type float32 and shape [None ,2] (None, i.e. any number of rows)
&amp;gt;&amp;gt;&amp;gt; y = tf.add(tf.matmul(x,W), b) # operation x.W + b
&amp;gt;&amp;gt;&amp;gt; s = tf.Session() # session created
&amp;gt;&amp;gt;&amp;gt; s.run(tf.initialize_all_variables()) # To initialize all the variables present in the current session
&amp;gt;&amp;gt;&amp;gt; s.run(y,feed_dict={x:[[1., 2.]]} # placeholder x feeded with value
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;: Operations in tensoflow are functions which applies some transformations to tensors on the computational graph. Like Variables and placeholders tensorflow operations can also be named for easy identification in the computational graph.An operation may consist of multiple kernels, for different types of devices.For example,  an operation may have seperate CPU and GPU kernels so that it can be excuted more efficiently on GPU.In the previous example &lt;strong&gt;tf.matmul()&lt;/strong&gt; and &lt;strong&gt;tf.add()&lt;/strong&gt; are two operations.As it relevant from the above example , tensorflow operations can be nested.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sessions&lt;/strong&gt;: A tensorflow session is responsible for interacting with the computational graph thus created and make necessary arrangements for execution of the graph.It allocates resources required by the computational graph and holds the values of variables.Tensorflow session object(say sess) is created using the &lt;strong&gt;tf.Session()&lt;/strong&gt; class.Finally we can the computational graph or a subpart of it using the &lt;strong&gt;sess.run()&lt;/strong&gt; method.All sess.run does is identify all the dependencies that compose the relevant subgraph, ensure that all the placeholder variables in the concerned subgraph are filled using feed_dict,and then start executing the subgraph.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
