{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scholarly import scholarly\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from parse_bib import *\n",
    "list_of_bibs=[]\n",
    "import arxiv\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading old dataset\n",
    "\n",
    "file1 = open(\"hateSpeechPapers.txt\",\"r\") \n",
    "list1=file1.readlines()\n",
    "file1.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_dict={}\n",
    "key=None\n",
    "for file in list1:\n",
    "    file=file.strip('\\n')\n",
    "    if(file[-1:]==\":\"):\n",
    "        key=file[:-1]\n",
    "    else:\n",
    "        name=file\n",
    "        if(name != \"\"):\n",
    "            try:\n",
    "                papers_dict[key].append(name)\n",
    "            except KeyError:\n",
    "                papers_dict[key]=[name]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set search_pub to True if u have keywords \n",
    "### if u have titles, set it to false\n",
    "search_pub=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cfa4ab86b1c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bib_folder/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_bibs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not dict"
     ]
    }
   ],
   "source": [
    "if search_pub==True:\n",
    "    search_name='Using Knowledge Graphs to improve Hate Speech Detection'\n",
    "    year_low = 2016\n",
    "    year_high =2022\n",
    "    temp=scholarly.search_pubs(search_name,year_low=year_low,year_high=year_high)\n",
    "    count=0\n",
    "\n",
    "    while(count<10):\n",
    "        try:\n",
    "            count=count+1\n",
    "            print(count)\n",
    "            temp_pub=next(temp,None)\n",
    "            if(temp_pub is None):\n",
    "                break\n",
    "            list_of_bibs.append(temp_pub['bib'])\n",
    "        except ConnectionRefusedError:\n",
    "            pass\n",
    "        filename = 'own_papers' +'_'+str(year_low)+'_'+str(year_high)+'.bib'\n",
    "        output_file = open('bib_folder/'+filename, 'w', encoding='utf-8')\n",
    "        for text in list_of_bibs:\n",
    "            output_file.write(text)\n",
    "        output_file.close()    \n",
    "else:\n",
    "    count=0\n",
    "    for key in papers_dict:\n",
    "        if(count!=4):\n",
    "            count+=1\n",
    "            continue\n",
    "            \n",
    "        list_of_bibs=[]\n",
    "        for paper in tqdm.tqdm(papers_dict[key][10:20]):\n",
    "            search_name=paper\n",
    "            temp=scholarly.search_pubs(search_name)\n",
    "            temp_pub=next(temp,None)\n",
    "            if(temp_pub is None):\n",
    "                    print(paper)\n",
    "                    pass\n",
    "            else:\n",
    "                list_of_bibs.append(temp_pub.bib)\n",
    "            if(len(list_of_bibs)!=0):\n",
    "                filename = key+'_from_old_repo.bib'\n",
    "                output_file = open('bib_folder/'+filename, 'w', encoding='utf-8')\n",
    "                for text in list_of_bibs:\n",
    "                    output_file.write(text)\n",
    "                output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'container_type': 'Publication',\n",
       " 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>,\n",
       " 'bib': {'title': 'Hate me, hate me not: Hate speech detection on facebook',\n",
       "  'author': [\"F Dell'Orletta\"],\n",
       "  'pub_year': '2017',\n",
       "  'venue': 'Proceedings of the First …',\n",
       "  'abstract': 'This improvement is particularly significant for the classification of the Hate class, with F-score of about 72%. These results pave the way to the employment of our system in a real-use context  Filtering offensive language in online communities using grammatical relations'},\n",
       " 'filled': False,\n",
       " 'gsrank': 1,\n",
       " 'pub_url': 'http://ceur-ws.org/Vol-1816/paper-09.pdf',\n",
       " 'author_id': ['', 'rJ1LGyYAAAAJ', 'uhInFTQAAAAJ'],\n",
       " 'num_citations': 161,\n",
       " 'url_scholarbib': '/scholar?q=info:i5zC6Yp5Hd4J:scholar.google.com/&output=cite&scirp=0&hl=en',\n",
       " 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUsing%2BKnowledge%2BGraphs%2Bto%2Bimprove%2BHate%2BSpeech%2BDetection%26hl%3Den%26as_sdt%3D0,33%26as_ylo%3D2016%26as_yhi%3D2022&citilm=1&json=&update_op=library_add&info=i5zC6Yp5Hd4J&ei=r_08YL2YFY6WywTUkra4Aw',\n",
       " 'citedby_url': '/scholar?cites=16005082288278903947&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       " 'url_related_articles': '/scholar?q=related:i5zC6Yp5Hd4J:scholar.google.com/&scioq=Using+Knowledge+Graphs+to+improve+Hate+Speech+Detection&hl=en&as_sdt=0,33&as_ylo=2016&as_yhi=2022',\n",
       " 'eprint_url': 'http://ceur-ws.org/Vol-1816/paper-09.pdf'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\\'title\\': \\'Hate me, hate me not: Hate speech detection on facebook\\', \\'author\\': [\"F Dell\\'Orletta\"], \\'pub_year\\': \\'2017\\', \\'venue\\': \\'Proceedings of the First …\\', \\'abstract\\': \\'This improvement is particularly significant for the classification of the Hate class, with F-score of about 72%. These results pave the way to the employment of our system in a real-use context  Filtering offensive language in online communities using grammatical relations\\'}]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(list_of_bibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists(\"author.json\"):\n",
    "    with open(\"author.json\", \"r\") as infile: \n",
    "        author_dict=json.load(infile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tags= ['english','arabic','italian','portugese','multilingual','german','french','indonesian','polish','kenyan','amharic','dutch']\n",
    "method_tags =['detection','generation','bias','counter','countering','survey','classification','classify','crime','dataset']\n",
    "type_of_hate_tags = ['racial','ethnic','offensive']\n",
    "\n",
    "\n",
    "reset_publications=False\n",
    "\n",
    "\n",
    "def get_author_link(author_name):\n",
    "    try:\n",
    "        author_web=author_dict[author_name]\n",
    "    except:\n",
    "        author_tuple=author_name.split(\" \")\n",
    "        author_plus_name=author_tuple[0]\n",
    "        for name in author_tuple[1:]:\n",
    "            author_plus_name=author_plus_name+'+'+name\n",
    "        author_web='https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors='+author_plus_name+'&btnG='\n",
    "        author_dict[author_name]=author_web\n",
    "    return author_web\n",
    "#the_file.write('url_pdf : \"/publication/'+entry['ID']+'/manuscript.pdf\"\\n'\n",
    "\n",
    "\n",
    "total_tags = language_tags+method_tags+type_of_hate_tags\n",
    "\n",
    "def bibtomark(inputfile,tags=None):\n",
    "    print(inputfile)\n",
    "    try:\n",
    "        with open(inputfile, encoding=\"utf8\") as bibtex_file:\n",
    "            bibtex_str = bibtex_file.read()\n",
    "#             print(bibtex_str)\n",
    "    except EnvironmentError:  # parent of IOError, OSError *and* WindowsError where available\n",
    "        print('File '+inputfile+' not found or some other error...')\n",
    "    \n",
    "    # It takes the type of the bibtex entry and maps to a corresponding category of the academic theme\n",
    "    # Publication type.\n",
    "    # Legend:\n",
    "    # 0 = Uncategorized\n",
    "    # 1 = Conference paper\n",
    "    # 2 = Journal article\n",
    "    # 3 = Preprint / Working Paper\n",
    "    # 4 = Report\n",
    "    # 5 = Book\n",
    "    # 6 = Book section\n",
    "    # 7 = Thesis\n",
    "    # 8 = Patent\n",
    "    pubtype_dict = {\n",
    "        'phdthesis': '\"7\"',\n",
    "        'mastersthesis': '\"7\"',\n",
    "        'Uncategorized': '\"0\"',\n",
    "        'inproceedings': '\"1\"',\n",
    "        'conference': '\"1\"',\n",
    "        'article': '\"2\"',\n",
    "        'submitted': '\"3\"',\n",
    "        'techreport': '\"4\"',\n",
    "        'book': '\"5\"',\n",
    "        'incollection': '\"6\"',\n",
    "    }\n",
    "    bib_database = bibtexparser.loads(bibtex_str)\n",
    "    #print(bib_database.entries)\n",
    "    for entry in bib_database.entries:\n",
    "        #print(entry)\n",
    "        filedir = 'content/publication/'+entry['ID'] \n",
    "        if not os.path.exists(filedir):\n",
    "            os.mkdir(filedir)\n",
    "        filenm = 'content/publication/'+entry['ID']+'/index.md'\n",
    "        \n",
    "        #If the same publication exists, then skip the creation. I customize the .md files later, so I don't want them overwritten. Only new publications are created.\n",
    "        if os.path.isfile(filenm) and reset_publications==False:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        with open(filenm, 'w', encoding='utf8') as the_file:\n",
    "            the_file.write('---\\n')\n",
    "            the_file.write('title: \"'+supetrim(entry['title'])+'\"\\n')\n",
    "            print('Parsing ' + entry['ID'])\n",
    "            \n",
    "            temp_list=arxiv.query(query=supetrim(entry['title']), max_results=1)\n",
    "            \n",
    "            if(len(temp_list)!=0 and temp_list[0]['title']!=supetrim(entry['title'])):\n",
    "                print('arxiv:',temp_list[0]['title'])\n",
    "                print('gscholar:',supetrim(entry['title']))\n",
    "                print('++++++++++++++')\n",
    "                temp_list=[]\n",
    "            \n",
    "            if 'year' in entry:\n",
    "                date = entry['year']\n",
    "                if 'month' in entry:\n",
    "                    if RepresentsInt(entry['month']):\n",
    "                        month = entry['month']\n",
    "                    else:\n",
    "                        month = str(month_string_to_number(entry['month']))\n",
    "                    date = date+'-'+ month.zfill(2)\n",
    "                else:\n",
    "                    date = date+'-01'\n",
    "                the_file.write('date: \"'+date+'-01\"\\n')\n",
    "                \n",
    "            # Treating the authors\n",
    "            if 'author' in entry:\n",
    "                authors = entry['author'].split(' and ')\n",
    "                #print(authors)\n",
    "                the_file.write('authors: [')\n",
    "                authors_str = ''\n",
    "                for author in authors:\n",
    "                    author_strip = supetrim(author)\n",
    "                    #print(author_strip)\n",
    "                    author_split = author_strip.split(',')\n",
    "                    if len(author_split)==2:\n",
    "                        author_strip = author_split[1].strip() + ' ' +author_split[0].strip()\n",
    "#                     author_split = author_strip.split(' ')\n",
    "#                     author_strip = author_split[0][0]+'. '+' '.join(map(str, author_split[1:]))\n",
    "                    if author_strip == 'll':\n",
    "                        author_strip = 'admin'\n",
    "                    author_web = get_author_link(author_strip)\n",
    "                    if author_web:\n",
    "                        #authors_str = author\n",
    "                        authors_str+='\"['+author_strip+'](' + author_web + ')\",'\n",
    "                    else:\n",
    "                        authors_str = authors_str+ '\"'+author_strip+'\",'\n",
    "            the_file.write(authors_str[:-1]+']\\n')\n",
    "            \n",
    "            # Treating the keywords\n",
    "            title_tokens=supetrim(entry['title']).lower().split(\" \")\n",
    "            \n",
    "            keywords=[]\n",
    "            if(tags!=None):\n",
    "                for temp in tags:\n",
    "                    keywords.append(temp)\n",
    "            \n",
    "            \n",
    "            flag_language = False\n",
    "            for tag in total_tags:\n",
    "                if tag in title_tokens:\n",
    "                    if(tag in language_tags):\n",
    "                        flag_language=True\n",
    "                    elif(tag == 'counter' or tag == 'countering' ):\n",
    "                        keywords.append('Counter speech')\n",
    "                    elif(tag == 'crime'):\n",
    "                        keywords.append('Hate crime')\n",
    "                        \n",
    "                    elif(tag in ['classification','classify','detection']):\n",
    "                        keywords.append('Detection')\n",
    "                    else:\n",
    "                        keywords.append(tag.capitalize())\n",
    "            \n",
    "            if(flag_language==False):\n",
    "                keywords.append('English')\n",
    "            #print(keywords)    \n",
    "                \n",
    "            if len(keywords) >0:\n",
    "                keywords=list(set(keywords))\n",
    "                the_file.write('tags : [')\n",
    "                keyword_str = ''\n",
    "                for keyword in keywords:\n",
    "                    keyword_str = keyword_str+ '\"'+keyword+'\",'\n",
    "                #print(keyword_str)\n",
    "                the_file.write(keyword_str[:-1]+']\\n')\n",
    "            else:\n",
    "                the_file.write('tags : []\\n')\n",
    "            \n",
    "            # Treating the publication type\n",
    "            if 'ENTRYTYPE' in entry:\n",
    "                if 'booktitle' in entry and ('Seminar' in supetrim(entry['booktitle'])):\n",
    "                    the_file.write('publication_types : ['+pubtype_dict['conference']+']\\n')\n",
    "                elif 'booktitle' in entry and ('Workshop' in supetrim(entry['booktitle'])):\n",
    "                    the_file.write('publication_types : ['+pubtype_dict['conference']+']\\n')\n",
    "                elif 'note' in entry and ('review' in supetrim(entry['note'])):\n",
    "                    the_file.write('publication_types : ['+pubtype_dict['submitted']+']\\n')\n",
    "                elif 'note' in entry and ('Conditional' in supetrim(entry['note'])):\n",
    "                    the_file.write('publication_types : ['+pubtype_dict['submitted']+']\\n')\n",
    "                else:\n",
    "                    the_file.write('publication_types : ['+pubtype_dict[entry['ENTRYTYPE']]+']\\n')\n",
    "            \n",
    "            # Treating the publication journal, conference, etc.\n",
    "            if 'booktitle' in entry:\n",
    "                the_file.write('publication : \"_'+supetrim(entry['booktitle'])+'_\"\\n')\n",
    "            elif 'journal' in entry:\n",
    "                the_file.write('publication : \"_'+supetrim(entry['journal'])+'_\"\\n')\n",
    "            elif 'school' in entry:\n",
    "                the_file.write('publication : \"_'+supetrim(entry['school'])+'_\"\\n')\n",
    "            elif 'institution' in entry:\n",
    "                the_file.write('publication : \"_'+supetrim(entry['institution'])+'_\"\\n')\n",
    "                \n",
    "            # I never put the short version. In the future I will use a dictionary like the authors to set the acronyms of important conferences and journals\n",
    "            the_file.write('publication_short : \"\"\\n')\n",
    "            \n",
    "            # Add the abstract if it's available in the bibtex\n",
    "            if 'abstract' in entry:\n",
    "                if(len(temp_list)==0):\n",
    "                    the_file.write('abstract : \"'+supetrim(entry['abstract'])+'\"\\n')\n",
    "                else:\n",
    "                    the_file.write('abstract : \"'+supetrim(temp_list[0]['summary'])+'\"\\n')\n",
    "            # Some features are disabled. I activate them later\n",
    "            the_file.write('summary : \"\"\\n')\n",
    "            if 'featured' in entry:\n",
    "                the_file.write('featured : true\\n')\n",
    "            else:\n",
    "                the_file.write('featured : false\\n')\n",
    "            the_file.write('projects : []\\n')\n",
    "            the_file.write('slides : \"\"\\n')\n",
    "\n",
    "            # I add urls to the pdf and the DOI venue = {Eleventh international aaai …},\n",
    "            #the_file.write('url_pdf : \"/publication/'+entry['ID']+'/manuscript.pdf\"\\n')\n",
    "            the_file.write('url_pdf : \"\"\\n')\n",
    "            if 'doi' in entry:\n",
    "                the_file.write('doi : \"'+supetrim(entry['doi'])+'\"\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            if(len(temp_list)!=0):\n",
    "                \n",
    "                try:\n",
    "                    url=re.search(\"(?P<url>https?://[^\\s]+)\", temp_list[0]['arxiv_comment']).group(\"url\")\n",
    "                    print(\"the code at,\",url)\n",
    "                    the_file.write('url_code : \"'+supetrim(url)+'\"\\n')\n",
    "                \n",
    "                except:\n",
    "                    the_file.write('url_code : \"\"\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            the_file.write('url_dataset : \"\"\\nurl_poster : \"\"\\n')\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                the_file.write('url_source : '+supetrim(entry['eprint'])+'\\n')\n",
    "            except KeyError:\n",
    "                the_file.write('url_source : \"\"\\n')\n",
    "                \n",
    "            the_file.write('url_slides : \"\"\\nurl_video : \"\"\\n')\n",
    "            \n",
    "            \n",
    "            # Default parameters that can be later customized\n",
    "            #the_file.write('math : true\\n')\n",
    "            #the_file.write('highlight : true\\n')\n",
    "            the_file.write('image :\\n')\n",
    "            the_file.write(' caption : \"\"\\n')\n",
    "            the_file.write(' focal_point: \"\"\\n')\n",
    "            the_file.write(' preview_only: false\\n')\n",
    "\n",
    "            \n",
    "            # I keep in my bibtex file a parameter called award for publications that received an award (e.g., best paper, etc.)\n",
    "            if 'award' in entry:\n",
    "                the_file.write('award : \"true\"\\n')\n",
    "            \n",
    "            # I put the individual .bib entry to a file with the same name as the .md to create the CITE option\n",
    "            db = BibDatabase()\n",
    "            db.entries =[entry]\n",
    "            writer = BibTexWriter()\n",
    "            with open('content/publication/'+entry['ID']+'/cite.bib', 'w', encoding='utf8') as bibfile:\n",
    "                bibfile.write(writer.write(db))\n",
    "\n",
    "            the_file.write('---\\n\\n')\n",
    "            the_file.write('### Main contributions\\n')\n",
    "            the_file.write('Coming soon \\n')\n",
    "            the_file.write('### Limitations\\n')\n",
    "            the_file.write('Coming soon \\n')\n",
    "            the_file.write('### Future Directions\\n')\n",
    "            the_file.write('Coming soon \\n')\n",
    "            \n",
    "            \n",
    "            # Any notes are copied to the main document\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "all_bib_files=glob.glob('bib_folder/own_papers*.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bib_folder/own_papers_2016_2021.bib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bib_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our papers']\n",
      "bib_folder/own_papers_2016_2021.bib\n",
      "Parsing saha2021short\n",
      "arxiv: \"Short is the Road that Leads from Fear to Hate\": Fear Speech in Indian\n",
      "  WhatsApp Groups\n",
      "gscholar: Short is the Road that Leads from Fear to Hate: Fear Speech in Indian WhatsApp Groups\n",
      "++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "for file in all_bib_files:\n",
    "    key=[file.split(\"/\")[1].split('_')[0]]\n",
    "    if(key==['hate speech'] or key ==['bias hate speech'] or key==['hate crime']):\n",
    "        key=None\n",
    "    if(key==['own']):\n",
    "        key=['Our papers']\n",
    "    \n",
    "    \n",
    "    print(key)\n",
    "    bibtomark(file,tags=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"author.json\", \"w\") as outfile: \n",
    "    outfile.write(json.dumps(author_dict,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bib_files[0].split(\"/\")[1].split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bib_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
